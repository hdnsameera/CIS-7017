{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop more robust and precise models that can identify potential churners early and provide actionable insights for retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTYtKVxjrRP"
   },
   "source": [
    "## The steps of the process of prediction used in this project\n",
    "\n",
    "- <b> < Data Collection > </b>:\n",
    "\n",
    "The dataset contains customer behavior, interactions, and historical churn.\n",
    "\n",
    "- <b> < Data Preprocessing > </b>:\n",
    "\n",
    "Clean and preprocess the data by handling missing values, outliers, and encoding categorical variables.         Making the dataset suitable for ML.\n",
    "     \n",
    "     \n",
    "- <b> < Feature Engineering ></b>:\n",
    "\n",
    "Create meaningful features that can help the model make accurate predictions.\n",
    "    This involves feature scaling, normalization, or generating new features based on domain knowledge.\n",
    "    \n",
    "    \n",
    "- <b> < Data Splitting > </b>:\n",
    "\n",
    "Split the dataset into training, validation, and test sets.\n",
    "    A common split is 80% for training, 20% for testing. The validation set is taken as a separate dataset and is used for hyperparameter tuning.\n",
    "    \n",
    "    \n",
    "- <b> < Model Selection > </b>:\n",
    "\n",
    "Choose the appropriate machine learning algorithms for churn prediction.\n",
    "    Common choices include K-nearest neighbors, support vector machine, logistic regression, random forests, decision trees, ada boost, gradient boosting, and voting.\n",
    "    Consider using ensemble methods or stacking multiple models for better performance.\n",
    "\n",
    "\n",
    "- <b> < Model Training > </b>:\n",
    "\n",
    "Train the selected models using the training dataset.\n",
    "    Tune hyperparameters to optimize model performance on the validation set.\n",
    "    This may involve techniques like cross-validation.\n",
    "\n",
    "\n",
    "- <b> < Model Evaluation > </b>:\n",
    "\n",
    "Evaluate the model's performance using appropriate metrics such as accuracy, precision, recall,\n",
    "    F1-score, ROC AUC, or customer-centric metrics like customer lifetime value (CLV).\n",
    "    Compare the performance of different models and\n",
    "    choose the one that best aligns with the business objectives.\n",
    "\n",
    "- <b> < Finalizing The Most Suitable Model > </b>\n",
    "\n",
    "Finalize the best model based on the metrics calculated.\n",
    "    \n",
    "- <b> < Feature Importance Analysis > </b>:\n",
    "\n",
    "Understand which features are most important for making churn predictions.\n",
    "    Feature importance analysis can help refine the model and provide insights into customer behavior.\n",
    "    \n",
    "    \n",
    "- <b> < Model Deployment > </b>:\n",
    "\n",
    "Once the model with satisfactory performance is selected, it can be deployed to a production environment where it can make real-time predictions. Consider using APIs or containerization for deployment.\n",
    "\n",
    "\n",
    "- <b> < Predicting the churn using the hold-out dataset > </b>:\n",
    "\n",
    "Feed in the hold-out dataset to the deployed model conduting all the activities applied during the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kewqJ0iEjrRQ"
   },
   "source": [
    "## Further steps to be followed after the model deployment\n",
    "\n",
    "- <b>Monitoring and Maintenance</b>:\n",
    "\n",
    "Continuously monitor the model's performance in a production environment. Re-train the model periodically with new data to ensure it remains accurate as customer behavior changes over time.\n",
    "    \n",
    "    \n",
    "- <b>Interpretability and Explainability</b>:\n",
    "\n",
    "Understand how the model is making predictions. Use techniques like SHAP values or LIME to interpret and explain model decisions to stakeholders.\n",
    "\n",
    "\n",
    "- <b>Feedback loop</b>:\n",
    "\n",
    "Incorporate feedback from business stakeholders, customer support, and other relevant sources to improve the model over time.\n",
    "    \n",
    "    \n",
    "- <b>Scale and Iterate</b>:\n",
    "\n",
    "As your business evolves and gathers more data, consider scaling the model and iterating on the process to improve prediction accuracy and reduce churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jloAKIAjrRR"
   },
   "source": [
    "## Libraries installed\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- plotly\n",
    "- kaleido\n",
    "- ydata-profiling (!pip install -U ydata-profiling)\n",
    "- seaborn\n",
    "- sklearn\n",
    "- jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raxQvmBdjrRR"
   },
   "source": [
    "## Special Notes\n",
    "\n",
    "##### Run the below code to upgrade '<b>threadpoolctl</b>' library if the training code of <b>KNN</b> model throws an exception.\n",
    "\n",
    "    - Mac: !pip install threadpoolctl --upgrade   \n",
    "    - Windows: pip install threadpoolctl --upgrade   \n",
    "\n",
    "\n",
    "##### Run the below code to install ydata-profiling  library\n",
    "    - Mac: !pip install -U ydata-profiling\n",
    "    - Windows: pip install -U ydata-profiling\n",
    "    \n",
    "##### Download plotly plots\n",
    "    - pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stHQOTWqjrRS"
   },
   "source": [
    "## Feature Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJLePsx_jrRV"
   },
   "source": [
    "## ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrsBlkg7jrRW"
   },
   "outputs": [],
   "source": [
    "# Libraries related to date and time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJLkklOMjrRX"
   },
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AT9wZuIG_ftz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initialize the program started time\n",
    "mainStartTime = datetime.now()\n",
    "\n",
    "# Initialize the ML start and end time.\n",
    "startTime = datetime.now()\n",
    "endTime = datetime.now()\n",
    "\n",
    "# List of the column names\n",
    "colNames = []\n",
    "\n",
    "# Run visualization codes (1=Show Plots and other graphs)\n",
    "showViz = 1\n",
    "\n",
    "# Create directories if not exist\n",
    "\n",
    "listFolders = ['distribution_bar', 'distribution_pie']\n",
    "\n",
    "for f in listFolders:\n",
    "\n",
    "    isExist = os.path.exists(f)\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAdtFhZQjrRY"
   },
   "source": [
    "##  < Data Collection >\n",
    "\n",
    "#### Two datasets are selected to the entire process.\n",
    "\n",
    "    - CustomerChurnDataset_TrainTest.csv - The dataset taken to train and test the models\n",
    "    - CustomerChurnDataset_Holdout.csv   - The dataset taken to predict the most accurate and appropriate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLz3JxYzjrRY"
   },
   "source": [
    "#### Importing the required libraries for data manupulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzYxVpVs-_t4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9ZvaGPrjrRZ"
   },
   "source": [
    "#### Reading the dataset into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0XY8uwGPJd9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('CustomerChurnDataset_TrainTest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1695231186246,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "ErCQwEabsIw9"
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1695231188890,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "c4pIF4NoOsJw",
    "outputId": "4145e4e3-c0eb-45c2-c2a9-86fde563cc65"
   },
   "outputs": [],
   "source": [
    "# Show the number of columns and rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View random sample of the dataset (5 observations)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "def showDataTypes():\n",
    "\n",
    "    dfDtypes = pd.DataFrame({'Data Type': [], 'Count': []})\n",
    "    count = 0\n",
    "    lDtypes = []\n",
    "    lCount = []\n",
    "\n",
    "    for x in df.dtypes.unique():\n",
    "        for y in df.dtypes:\n",
    "            if x == y:\n",
    "                count = count + 1\n",
    "        lDtypes.append(x)\n",
    "        lCount.append(count)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "    dfDtypes['Data Type'] = lDtypes\n",
    "    dfDtypes['Count'] = lCount\n",
    "\n",
    "    return dfDtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing data types\n",
    "showDataTypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'City' column\n",
    "df['City'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are 1129 unique categorical values in the 'City' column, one-hot coding cannot be applied.\n",
    "# In order to avoid complications in the process, the 'City' column shoudl be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CustomerID', 'Churn Score' and 'Churn Reason' are not considered dependent variables\n",
    "df.drop(['CustomerID', 'City', 'Churn Score', 'Churn Reason'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing data types\n",
    "showDataTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in 'Churn Label' and 'Churn Value' features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn Value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate 'Yes' and 'No' values in 'Churn Label' match with 1 and 0 in 'Churn Value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the total = 7043 (total of observations), one features can be dropped. (In this case, the 'Churn Label' is to be dropped)\n",
    "\n",
    "count1 = len(df.loc[(df['Churn Label'] == 'Yes') & (df['Churn Value'] == 1)])\n",
    "count2 = len(df.loc[(df['Churn Label'] == 'No') & (df['Churn Value'] == 0)])\n",
    "\n",
    "count1 + count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the 'Churn Value' as the dependent variable and to be predicted value. For the sake of ease, the\n",
    "# name of the column get changed as 'Churn'\n",
    "\n",
    "df.rename(columns = {'Churn Value':'Churn'}, inplace = True)\n",
    "\n",
    "# Drop the 'Churn Label'\n",
    "df.drop('Churn Label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695231192219,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "LTEvrifBi-HJ",
    "outputId": "dd8ec312-e5bf-4916-9a30-71175492eba3"
   },
   "outputs": [],
   "source": [
    "# Check for unique values in dtype 'object' (categorical)\n",
    "\n",
    "def showUniqueValues(df):\n",
    "  colNames.clear()\n",
    "  for column in df:\n",
    "    colNames.append(column)\n",
    "    if df[column].dtype =='object':\n",
    "      print(f'{column} : {df[column].unique()}')\n",
    "\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5VepUeJjrRd",
    "outputId": "d8dfa506-7bd8-4cb3-ea34-f7df7abd612c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of column names\n",
    "colNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1695231192800,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "Er56FN7-uVCm"
   },
   "source": [
    "## < Data preprocessing >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIhuwCFejrRh"
   },
   "source": [
    "#### Function to check for missing data or NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_selector = df.isnull().any(axis=1)\n",
    "null_row_count = df[null_rows_selector].shape[0]\n",
    "\n",
    "df_null = df.isnull().groupby(df['Churn']).sum().transpose()\n",
    "df_null['total'] = df.isnull().sum()\n",
    "df_null['percent'] = (df_null['total']/len(df))*100\n",
    "df_null = df_null[df_null.total!=0]\n",
    "\n",
    "print(\"rows with null values:\",null_row_count,\", {:.2f}%\".format((null_row_count/len(df))*100))\n",
    "print('columns with null values:',df_null.shape[0])\n",
    "\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the correlation with the 'Churn' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi square independence test to see if the difference in distributions is statistically significant\n",
    "\n",
    "contingency_table = pd.concat([df['Churn'].value_counts().rename(\"Overall\"), df[null_rows_selector]['Churn'].value_counts().rename(\"within_null_rows\")],axis=1).transpose()\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df.columns:\n",
    "    if df[x].isna().sum() != 0:\n",
    "        print(x, df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 'Churn Reason' column contains the null values. By observing the above results, it is identified that \n",
    "# the number of null values equals the number of rows where 'Churn Value' = 0. Since the 'Churn Reason' is not considered\n",
    "# in this research, there are no null values in the dataset after dropping the 'Churn Reason' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency,ttest_ind\n",
    "\n",
    "# chi square independence test\n",
    "# Null Hypothesis HO: Distribution of Churn is independent of presence of null values\n",
    "\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(contingency_table.values)\n",
    "  \n",
    "# interpret p-value\n",
    "alpha = 0.05 # significance value for test\n",
    "print('p value is ' + str(p))\n",
    "\n",
    "print('Dependent (reject H0)') if p <= alpha else print('Independent (H0 holds true)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowCorrelationFeatures = []\n",
    "# Categorical low correlation features\n",
    "\n",
    "lowCorrelationFeaturesC = []\n",
    "\n",
    "# Numerical low correlation features\n",
    "lowCorrelationFeaturesN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi squared Independence test between categorical values and Churn. This will help to identify important variables on which Churn depends.\n",
    "# Based on the test, we can recommend to drop/include them in while training\n",
    "def chi2test(X,y,alpha=0.05):\n",
    "    '''\n",
    "        X : dataframe \n",
    "        y : series\n",
    "    '''\n",
    "    target = y.name\n",
    "    print('ch2test with alpha',alpha)\n",
    "    test_df = []\n",
    "    for index,col in X.select_dtypes(include=['object']).columns.to_series().items():\n",
    "        df = pd.concat([y,X[col]],axis=1)\n",
    "        contingency_table = df.value_counts().rename(\"counts\").reset_index().pivot(index=target,columns=col,values='counts').fillna(0)\n",
    "        stat, p, dof, expected = chi2_contingency(contingency_table.values)\n",
    "        test_df.append([target,col,stat,p,'Dependent (reject H0)' if p <= alpha else 'Independent (H0 holds true)','include' if p <= alpha else 'drop'])\n",
    "        \n",
    "    test_df = pd.DataFrame(test_df,columns=[\"variable1\",\"variable2\",\"chi2-stat\",\"p-value\",\"result\",\"recommendation\"])\n",
    "    return test_df\n",
    "\n",
    "df_chi = chi2test(df.drop('Churn',axis=1),df['Churn'])\n",
    "\n",
    "lowCorrelationFeaturesC.extend(df_chi[df_chi.recommendation == 'drop']['variable2'].to_list())\n",
    "\n",
    "df_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print values of lowCorrelationFeatures in one line.\n",
    "\n",
    "def fnPrintLowCorrelationFeatures(l1):\n",
    "    \n",
    "    n = len(l1)\n",
    "\n",
    "    for i in l1:\n",
    "        if i != l1[n-1]:\n",
    "            print(i, end= ', ')\n",
    "        else:\n",
    "            print(i, end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the list of categorical features with low correlation\n",
    "fnPrintLowCorrelationFeatures(lowCorrelationFeaturesC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t test to check if means of a numerical variable differ significantly if Churn is different. \n",
    "\n",
    "def t_test(X,y,alpha=0.05):   \n",
    "    target = y.name\n",
    "    print('t_test with alpha',alpha)\n",
    "    test_df = []\n",
    "    for index,col in X.select_dtypes(exclude=['object']).columns.to_series().items():\n",
    "        df = pd.concat([y,X[col]],axis=1)\n",
    "        ttest_df = df.set_index(target,drop=True).fillna(0)\n",
    "        stat, p = ttest_ind(ttest_df.loc[1],ttest_df.loc[0],equal_var=False)\n",
    "        test_df.append([target,col,stat,p,'Dependent (reject H0)' if p <= alpha else 'Independent (H0 holds true)','include' if p <= alpha else 'drop'])\n",
    "        \n",
    "    test_df = pd.DataFrame(test_df,columns=[\"variable1\",\"variable2\",\"t-stat\",\"p-value\",\"result\",\"recommendation\"])\n",
    "    return test_df\n",
    "\n",
    "df_t_test = t_test(df.drop('Churn',axis=1),df['Churn'])\n",
    "\n",
    "lowCorrelationFeaturesN.extend(df_t_test[df_t_test.recommendation == 'drop']['variable2'].to_list())\n",
    "\n",
    "df_t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the list of columns to be dropped\n",
    "lowCorrelationFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the list of numerical features with low correlation\n",
    "fnPrintLowCorrelationFeatures(lowCorrelationFeaturesN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge categorical and numerical features with low correlation\n",
    "lowCorrelationFeatures = lowCorrelationFeaturesC + lowCorrelationFeaturesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the list of all the features with low correlation\n",
    "fnPrintLowCorrelationFeatures(lowCorrelationFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(lowCorrelationFeatures, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyize categorical type variables\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'No internet service' and 'No phone service' values with 'No' in the entire dataset\n",
    "df.replace('No internet service', 'No', inplace=True)\n",
    "df.replace('No phone service', 'No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyize categorical type variables\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4snS0090jrRm"
   },
   "source": [
    "### NA imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA imputation is not required for this dataset because there are now null values avaiable except for -\n",
    "# - the 'Churn Reason'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB2goKWqjrRs"
   },
   "source": [
    "## Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five-Number Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr26HA11jrRs"
   },
   "source": [
    "### Data distribution - Pie charts\n",
    "#### Churn visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695231208413,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "WcNUcQMXcuW3",
    "outputId": "d5375b24-ee54-4318-e9f1-62f32b0f21a7"
   },
   "outputs": [],
   "source": [
    "import plotly.offline as po\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "if showViz == 1:\n",
    "    churn_key = df['Churn'].value_counts().keys().tolist()\n",
    "    churn_value = df['Churn'].value_counts().values.tolist()\n",
    "\n",
    "    plot_data = [\n",
    "        go.Pie(labels=churn_key, values=churn_value, marker=dict(colors=['Teal','Gray'], line=dict(color='white', width=1.5)),\n",
    "        rotation=90,\n",
    "        hoverinfo=\"label+value+text\",\n",
    "        hole=0.6)\n",
    "    ]\n",
    "\n",
    "    plot_layout = go.Layout(dict(title=\"Customer Churn\", plot_bgcolor='rgb(243, 243,243)', paper_bgcolor='rgb(243, 243, 243)',))\n",
    "\n",
    "    fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "    \n",
    "    po.plot(fig, filename = f'{listFolders[1]}/Customer Churn.html', auto_open=False)\n",
    "    \n",
    "    po.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical type features distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695231210721,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "f7C1BYxTcugS",
    "outputId": "f6a5f58d-30de-46ca-ad0f-d7f1d568006b"
   },
   "outputs": [],
   "source": [
    "# The following function is used to visualize the distribution of the other columns\n",
    "\n",
    "# Function to visualize the distribution\n",
    "def distributionPie(column):\n",
    "  labels = df[column].unique()\n",
    "  values = df[column].value_counts()\n",
    "\n",
    "  fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=0.6, rotation=90)])\n",
    "  fig.update_layout(title_text=f\"{column} Distribution\", plot_bgcolor='rgb(243, 243,243)')\n",
    "  po.plot(fig, filename = f'{listFolders[1]}/{column}.html', auto_open=False)\n",
    "  fig.show()\n",
    "\n",
    "if showViz == 1:\n",
    "    # Loop the column names (categorical)\n",
    "#     for column in df.drop(['ServiceArea', 'Churn', 'HandsetPrice'], axis=1):\n",
    "    \n",
    "#     for column in df.drop(['Churn', 'HandsetPrice'], axis=1):\n",
    "#         if df[column].dtype =='object':\n",
    "#             distributionPie(column)\n",
    "            \n",
    "    for column in df.drop(['Churn'], axis=1):\n",
    "        if df[column].dtype =='object':\n",
    "            distributionPie(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwyPF4ksjrRu"
   },
   "source": [
    "### Pandas profiling report\n",
    "\n",
    "##### This report is an interactive data analysis report for quickly gaining insights into a dataset. The report provides a wide range of statistical and visual summaries of the data, helping data analysts and data scientists to understand the dataset's characteristics, identify potential issues, and make informed decisions about data preprocessing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXuzQkl0jrRu"
   },
   "source": [
    "#### Generating the profiling report and saving as an HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujIPp54KjrRv"
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "def runProfilingReport(fName):\n",
    "    \n",
    "    # ProfileReport started time\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    from ydata_profiling import ProfileReport\n",
    "\n",
    "    # Create pandas profiling report\n",
    "    profReport = ProfileReport(df)\n",
    "\n",
    "    # Download pandas profiling report in html format\n",
    "    profReport.to_file(f'{fName}.html')\n",
    "\n",
    "    endTime = datetime.now()\n",
    "    \n",
    "    # Open the html file\n",
    "    filename = 'file:///'+os.getcwd()+'/' + f'{fName}.html'\n",
    "    webbrowser.open(filename , new=2)\n",
    "\n",
    "    print(f'Profile Report processing time : {endTime-startTime}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    runProfilingReport('Row Data Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmY_zWNQjrRv"
   },
   "source": [
    "#### View the profiling report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKQwI3dkjrRv"
   },
   "outputs": [],
   "source": [
    "# View pandas profiling report (This will take a few minutes to load the report)\n",
    "# Since this takes a considerable amount of time to load, the report is saved as 'Row Data Analysis.html' in the directory.\n",
    "# Open the exported html file to view the plots and other statistics instead.\n",
    "\n",
    "# Uncomment the below code to view the report along with the code.\n",
    "\n",
    "#profReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJuV_zIUjrRw"
   },
   "source": [
    "#### Visualizing the distribution of numerical features by 'Churn' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on0AdJCDjrRw",
    "outputId": "de908759-ea10-4f65-cab8-732189f243bb"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "if showViz == 1:\n",
    "\n",
    "    for column in df.drop('Churn', axis=1):\n",
    "        \n",
    "        if df[column].dtype !='object':\n",
    "            fig, ax = plt.subplots(figsize=(6, 3))\n",
    "            sns.set_context(\"paper\",font_scale=1.1)\n",
    "            ax = sns.kdeplot(df[column][(df[\"Churn\"] == 0) ],\n",
    "                            color=\"Red\", fill=True);\n",
    "            ax = sns.kdeplot(df[column][(df[\"Churn\"] == 1) ],\n",
    "                            ax =ax, color=\"Blue\", fill=True);\n",
    "            ax.legend([\"Not Churn\",\"Churn\"],loc='upper right');\n",
    "            ax.set_ylabel('Density');\n",
    "            ax.set_xlabel(column);\n",
    "            ax.set_title(f'Distribution of {column} by churn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1695231218398,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "tocGkMH18yVh"
   },
   "source": [
    "#### Comparing the features statistically against 'Churn' values 'Yes' and 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1695231220289,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "9fh8jiqlmQIA",
    "outputId": "1b813f02-a199-4a49-d522-43f1be4c5708"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def compareStatsWithChurn(colName):\n",
    "\n",
    "  fig = px.box(df, x='Churn', y = colName)\n",
    "\n",
    "  # Update yaxis properties\n",
    "  fig.update_yaxes(title_text=colName, row=1, col=1)\n",
    "  # Update xaxis properties\n",
    "  fig.update_xaxes(title_text='Churn', row=1, col=1)\n",
    "\n",
    "  # Update size and title\n",
    "  fig.update_layout(autosize=True, width=750, height=600,\n",
    "      title_font=dict(size=25, family='Courier'),\n",
    "      title=f'<b>{colName} vs Churn</b>',\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "if showViz == 1 :\n",
    "    for c in colNames:\n",
    "        compareStatsWithChurn(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVhHugSijrRx"
   },
   "source": [
    "#### Analyzing 'Churn' vs other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5v-i37lYFfK"
   },
   "outputs": [],
   "source": [
    "# Function to analyze customer churn vs other features\n",
    "\n",
    "def churnHist(colName):\n",
    "  fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "  churn_yes = df[df.Churn==1][colName]\n",
    "  churn_no = df[df.Churn==0][colName]\n",
    "\n",
    "  ax.hist([churn_yes, churn_no], color=['red','purple'], label=['Yes','No'])\n",
    "  ax.legend()\n",
    "\n",
    "  ax.set(title=f'Customer churn vs {colName} analysis', xlabel=colName, ylabel='Number of customers')\n",
    "\n",
    "  plt.savefig(f'{listFolders[0]}/{column}.png', dpi=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 92455,
     "status": "ok",
     "timestamp": 1695213391734,
     "user": {
      "displayName": "Nuwan Sameera",
      "userId": "07240712498791241499"
     },
     "user_tz": -330
    },
    "id": "Ilq4fhIsYasa",
    "outputId": "d681d8e0-c61d-4062-e253-8f4061057a75"
   },
   "outputs": [],
   "source": [
    "# Customer churn vs other features\n",
    "\n",
    "if showViz == 1:\n",
    "    for column in df.drop('Churn', axis=1):\n",
    "      churnHist(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTO6c049jrRz"
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show categorical unique values\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl9sp79bjrR0"
   },
   "source": [
    "#### Encoding Type 1 : Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Zz-1Fc0Reeb"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "labelEncodedColumns = ['Senior Citizen','Partner','Dependents','Multiple Lines','Online Security',\n",
    "                       'Online Backup','Device Protection','Tech Support','Streaming TV','Streaming Movies',\n",
    "                       'Contract','Paperless Billing']\n",
    "\n",
    "def labelEncoding(df):\n",
    "    \n",
    "    # Contract feature contains ordinal categorical values. In order to give an order, the 'Contract' values \n",
    "    # 'Month-to-month', 'Two year' and 'One year' are replaced with 0, 1 and 2\n",
    "\n",
    "#     df['Contract'] = df['Contract'].map({'Month-to-month':0, 'One year':1, 'Two year':2})\n",
    "\n",
    "    # Defining an array to store the names of the columns where the values contain 'Yes' and 'No' only.\n",
    "    \n",
    "    # Loop to do label encoding\n",
    "    for col in labelEncodedColumns:\n",
    "        df[col]= label_encoder.fit_transform(df[col])  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show categorical unique values\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GndMwhVjUCIw"
   },
   "outputs": [],
   "source": [
    "# Check shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in labelEncodedColumns:\n",
    "    print(f'{c} : ', df[c].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLGgzIC5jrR2"
   },
   "source": [
    "#### Encoding Type 2 : One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJPWLwJgUqRS"
   },
   "outputs": [],
   "source": [
    "def oneHotEncoding(df):\n",
    "    # One hot encoding is done to the relevant columns at once.\n",
    "    df = pd.get_dummies(data=df, columns=['Internet Service', 'Payment Method'], dtype=float)\n",
    "    return df\n",
    "\n",
    "df = oneHotEncoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Rk_vovo6jT7"
   },
   "outputs": [],
   "source": [
    "# View the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6YOOl5HURRJ"
   },
   "outputs": [],
   "source": [
    "# Show unique values of the columns\n",
    "showUniqueValues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvJklLpgjrR4"
   },
   "outputs": [],
   "source": [
    "# Get the column names\n",
    "colNames.clear()\n",
    "for column in df:\n",
    "    colNames.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpVS4JpoR3x2"
   },
   "outputs": [],
   "source": [
    "# Show some statistics\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0qRUrCj_9Qq"
   },
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2v7gB3dMjrR5"
   },
   "outputs": [],
   "source": [
    "len(colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdXnI7TDjrR6"
   },
   "source": [
    "### Treating Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn7TYA3WjrR6"
   },
   "source": [
    "#### Function to visualize outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts_OpTozjrR6"
   },
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "# Since the number of features is 73, the boxplots are grouped as 5 features per group.\n",
    "import math\n",
    "\n",
    "def showBoxplots2():\n",
    "    \n",
    "  l1 = []\n",
    "  j = 0\n",
    "\n",
    "  for i in range(math.ceil(len(colNames)/20)):\n",
    "    l1.append(colNames[i*20:(i+1)*20])\n",
    "    #df.boxplot(column=colNames[i*5:(i+1)*5])\n",
    "\n",
    "  for i in l1:\n",
    "    j = j + 1\n",
    "    fig = plt.subplots()\n",
    "    b_plot = df.boxplot(column=i, vert=False)\n",
    "    #df.T.boxplot(vert=False)\n",
    "    #plt.subplots_adjust(left=0.25)\n",
    "    b_plot.plot()\n",
    "    #plt.xticks(rotation=90)\n",
    "    plt.savefig(f'boxplots/{j}.png', dpi=100);\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showBoxplots(title):\n",
    "    \n",
    "    fig = plt.subplots(figsize=(10, 8))\n",
    "    b_plot = df.drop('Churn', axis=1).boxplot(vert=False)\n",
    "    b_plot.plot()\n",
    "    plt.title(f'Boxplots - {title}', fontsize = 16, weight = 'extra bold')\n",
    "    #plt.savefig(f'Boxplots - {title}.png', dpi=200);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibknBmUWjrR7"
   },
   "source": [
    "#### Show boxplots - Before treating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAXLGAGujrR7"
   },
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    showBoxplots('Before Treating Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYbcSdMNjrR8"
   },
   "source": [
    "#### Removing outliers\n",
    "\n",
    "The outliers are replaced with the median\n",
    "In does not display having outliers in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_T3g8BujrR8"
   },
   "outputs": [],
   "source": [
    "colNames.remove('Churn')\n",
    "\n",
    "for column in colNames:\n",
    "\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    upperLevel = q3 + (iqr * 1.5)\n",
    "    lowerLevel = q1 - (iqr * 1.5)\n",
    "\n",
    "    df[column][df[column] < lowerLevel] = df[column][df[column] > upperLevel] = df[column].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7jojUdZjrR8"
   },
   "source": [
    "#### Show boxplots - After treating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwkZFB8AjrR9"
   },
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    #colNames.remove('Churn')\n",
    "    showBoxplots('After Treating Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate lasso coefficient plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# df['Churn'].replace({'Yes':1, 'No':0}, inplace=True)\n",
    "\n",
    "def lassoCoefficientGraph():\n",
    "    \n",
    "#     df['Churn'].replace({'Yes':1, 'No':0}, inplace=True)\n",
    "\n",
    "    # Create a Lasso regression model with a specific alpha (regularization strength)\n",
    "    lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "    lasso_model.fit(df, df['Churn'])\n",
    "\n",
    "    # Access the Lasso coefficients\n",
    "    lasso_coefficients = lasso_model.coef_\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "    ax.bar(df.columns, lasso_coefficients)\n",
    "    ax.set(title='Lasso Coefficient Graph', xlabel='Features', ylabel='')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    plt.xticks(rotation=70);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso coefficient graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCoefficientGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categoricals\n",
    "categoricals = list()\n",
    "for x in df.columns:\n",
    "    if df[x].dtype == 'object':\n",
    "        categoricals.append(x)\n",
    "df[categoricals].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = [x for x in df.columns if x not in categoricals]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "df[numericals].corr(method='pearson')['Churn'].sort_values(ascending = False).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Feature Engineering >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzKDvfxojrR9"
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGip7og87ZN8"
   },
   "outputs": [],
   "source": [
    "# Check for features with type 'object'\n",
    "\n",
    "for column in df:\n",
    "  if df[column].dtype =='object':\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAes5yGkjrR-"
   },
   "source": [
    "#### Min_Max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ0IDi7GBeFW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaPDH2x1QVwy"
   },
   "outputs": [],
   "source": [
    "def minMaxScale(df):\n",
    "    # Get all the column names to a list.\n",
    "    colNames = df.columns.tolist()[0:len(df.columns.tolist())]\n",
    "\n",
    "    # Apply Min-Max scaler\n",
    "    df[colNames] = scaler.fit_transform(df[colNames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns5gB_nMjrR_"
   },
   "source": [
    "#### Apply Min-Max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v41Nl1-uQ7Dl"
   },
   "outputs": [],
   "source": [
    "# Call min-max scaling function\n",
    "minMaxScale(df)\n",
    "\n",
    "# View sample\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYDhK7UOjrSA"
   },
   "source": [
    "###### Show some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dF9Kww9gq1Nt"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rFASaXhjrSB"
   },
   "source": [
    "#### Show boxplots - After scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SVwKGYAjrSB"
   },
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    #colNames.remove('Churn')\n",
    "    showBoxplots('After Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The boxplots generated after feature scaling indicates the distribution of some features are very low and near to zero. To make more sense of these columns, the first model is training process is done with all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOFKIEA9jrSB"
   },
   "source": [
    "## < Data Splitting >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC7L5PFbSQON"
   },
   "source": [
    "### Train & Test Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKZBcM9DTBNh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = X_test = df.drop(df.index)\n",
    "y_train = y_test = df.drop(df.index)\n",
    "\n",
    "def splitDataset(df):\n",
    "    # All the columns except 'Churn'\n",
    "    X = df.drop('Churn', axis=1)\n",
    "\n",
    "    # 'Churn' column\n",
    "    y = df['Churn']\n",
    "\n",
    "    # Access dataframes declared globaly\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0FX5VMGT69U"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loew9MtjW2KM"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c91R6fCjrSD"
   },
   "source": [
    "## < Model Selection >\n",
    "\n",
    "The below supervised learning machine learning algorithms are used to analyze the customer churn dataset.\n",
    "\n",
    "- #### K-Nearest Neighbors (KNN)\n",
    "- #### Support Vector Machine (SVM)\n",
    "- #### Random Forest\n",
    "- #### Logistic Regression\n",
    "- #### Decision Tree Classifier\n",
    "- #### Ada Boost Classifier\n",
    "- #### Gradient Boosting Classifier\n",
    "- #### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0_DCV-6jrSD"
   },
   "source": [
    "### Customized functions used to calculate important parameters for ML selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUz5VbDujrSD"
   },
   "source": [
    "#### Calculating and storing the duration spent for ML processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF187v5Ux90N"
   },
   "outputs": [],
   "source": [
    "# DataFrame to select the appropriate ML algorithm for this customer churn prediction dataset\n",
    "df_ml_eval = pd.DataFrame({'ATTEMPT':[], 'ALGORITHM':[], 'TIME':[]})\n",
    "\n",
    "attempt = 0\n",
    "\n",
    "algoList = ['KNN', 'SVM', 'Random Forest', 'Logistic Regression', 'Decision Tree', 'Ada Boost', 'Gradient Boosting', 'Voting']\n",
    "\n",
    "# Function to calculate ML processing time and results, and store records in the 'dfTimeML' DataFrame for analysis\n",
    "def calculateTimeML(t, nameML=None, accuracy=None):\n",
    "  global startTime\n",
    "  global endTime\n",
    "\n",
    "  if t == 1:\n",
    "     startTime = datetime.now()\n",
    "  elif t == 2:\n",
    "     endTime = datetime.now()\n",
    "\n",
    "\n",
    "     # Print duration\n",
    "     print(f'{nameML} time : {(endTime-startTime).total_seconds()}')\n",
    "\n",
    "     # Store time in the DataFrame\n",
    "     df_ml_eval.loc[len(df_ml_eval.index)+1] = [attempt, nameML, (endTime-startTime).total_seconds()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9R5vxdBjrSE"
   },
   "source": [
    "#### Function to generate Confusion Matrix Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VBigXYajrSF"
   },
   "outputs": [],
   "source": [
    "def generateConfusionMatrixGraph(algo, y_test, pred, attempt):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "#     plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(confusion_matrix(y_test, pred, labels=[0, 1]), cmap=\"Blues\",\n",
    "                    annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set(title=f'{algo} Confusion Matrix')\n",
    "    \n",
    "    plt.savefig(f'Confusion Matrix {attempt} - {algo}.png', dpi=200)\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W8IL9JdjrSF"
   },
   "source": [
    "#### Function to generate Confusion Matrix Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGNMsgwajrSF"
   },
   "outputs": [],
   "source": [
    "def generateConfusionMatrixArray(prediction):\n",
    "    # Assigned the actual and predicted values to a dictionary.\n",
    "    dvalues = {'y_actual': y_test, 'y_predicted': prediction}\n",
    "\n",
    "    # Create a dataframe from dvalues dictionary.\n",
    "    dfcm = pd.DataFrame(dvalues)\n",
    "\n",
    "    # Create the confusion matrix using the dfcm dataframe.\n",
    "    cm = pd.crosstab(dfcm['y_actual'], dfcm['y_predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing 'Yes' and 'No' values with 1 and 0 in y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceYesNo():\n",
    "    # Number of model training attempts\n",
    "    global attempt\n",
    "    \n",
    "    attempt = attempt + 1\n",
    "\n",
    "    # Replacing 'Yes' and 'No' values with 1 and 0 to avoid python errors\n",
    "    y_test.replace({'Yes':1, 'No':0}, inplace=True)\n",
    "    y_train.replace({'Yes':1, 'No':0}, inplace=True)\n",
    "\n",
    "replaceYesNo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1dwprsmjrSG"
   },
   "source": [
    "## < Model Training >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZsENuoSnKXC"
   },
   "source": [
    "### 01 : K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1RcEHgnOFc"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def trainModel_knn():\n",
    "    \n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_knn = KNeighborsClassifier(n_neighbors = 11)\n",
    "    model_knn.fit(X_train,y_train)\n",
    "    prediction_knn = model_knn.predict(X_test)\n",
    "    accuracy_knn = model_knn.score(X_test,y_test)\n",
    "    print(\"KNN accuracy :\",accuracy_knn)\n",
    "\n",
    "    calculateTimeML(2, 'KNN', accuracy_knn)\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test, prediction_knn))\n",
    "    \n",
    "    return (model_knn, prediction_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbguaIzJnyuM"
   },
   "source": [
    "### 02 : Support Vector Machine - SVM\n",
    "\n",
    "##### Since the Support Vector Machine takes a considerable amount of time compared to the other algorithms, SVM was not considered in this research form here onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "A9k-eItPnzR4"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def trainModel_svc():\n",
    "    \n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_svc = SVC(random_state = 1, probability=True)\n",
    "    model_svc.fit(X_train,y_train)\n",
    "    prediction_svc = model_svc.predict(X_test)\n",
    "    accuracy_svc = model_svc.score(X_test,y_test)\n",
    "    print('SVM accuracy :',accuracy_svc)\n",
    "\n",
    "    calculateTimeML(2, 'SVM', accuracy_svc)\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test, prediction_svc))\n",
    "\n",
    "    return (model_svc, prediction_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJmr4I8Wq7q1"
   },
   "source": [
    "### 03 : Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JuB9fW9iq-3t"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def trainModel_rf():\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,\n",
    "                                      random_state =50, max_features = \"sqrt\",\n",
    "                                      max_leaf_nodes = 30)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    prediction_rf = model_rf.predict(X_test)\n",
    "    accuracy_rf = metrics.accuracy_score(y_test, prediction_rf)\n",
    "    print('Random Forest accuracy :', accuracy_rf)\n",
    "\n",
    "    calculateTimeML(2, 'Random Forest', accuracy_rf)\n",
    "\n",
    "    print(\"\\nClassification Report\")\n",
    "    print(classification_report(y_test, prediction_rf))\n",
    "    \n",
    "    return (model_rf, prediction_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZqFghwMrtoP"
   },
   "source": [
    "###  04 : Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rOedKGjOrtsR"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def trainModel_lr():\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(X_train,y_train)\n",
    "    prediction_lr = model_lr.predict(X_test)\n",
    "    accuracy_lr = model_lr.score(X_test,y_test)\n",
    "    print(\"Logistic Regression accuracy :\",accuracy_lr)\n",
    "\n",
    "    calculateTimeML(2, 'Logistic Regression', accuracy_lr)\n",
    "\n",
    "    print(\"\\nClassification Report\")\n",
    "    print(classification_report(y_test,prediction_lr))\n",
    "    \n",
    "    return (model_lr, prediction_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEfTnLMYtgYT"
   },
   "source": [
    "### 05 : Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "u3duDOqbtgb6"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def trainModel_dt():\n",
    "\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_dt = DecisionTreeClassifier()\n",
    "    model_dt.fit(X_train,y_train)\n",
    "    prediction_dt = model_dt.predict(X_test)\n",
    "    accuracy_dt = model_dt.score(X_test,y_test)\n",
    "    print(\"Decision Tree accuracy is :\",accuracy_dt)\n",
    "\n",
    "    calculateTimeML(2, 'Decision Tree', accuracy_dt)\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test, prediction_dt))\n",
    "    \n",
    "    return (model_dt, prediction_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIj00mGktgjf"
   },
   "source": [
    "### 06 : Ada Boost Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CMJqG2y8tgnL"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def trainModel_abc():\n",
    "\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_abc = AdaBoostClassifier()\n",
    "    model_abc.fit(X_train,y_train)\n",
    "    prediction_abc = model_abc.predict(X_test)\n",
    "    accuracy_abc = metrics.accuracy_score(y_test, prediction_abc)\n",
    "    print(\"Ada Boost Classifier accuracy : \", accuracy_abc)\n",
    "\n",
    "    calculateTimeML(2, 'Ada Boost', accuracy_abc)\n",
    "\n",
    "    print('\\nClassificatin Report')\n",
    "    print(classification_report(y_test, prediction_abc))\n",
    "    \n",
    "    return (model_abc, prediction_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU-M1a2CtgzV"
   },
   "source": [
    "### 07 : Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OdLx9vRvuL0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def trainModel_gbc():\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    model_gbc = GradientBoostingClassifier()\n",
    "    model_gbc.fit(X_train, y_train)\n",
    "    prediction_gbc = model_gbc.predict(X_test)\n",
    "    accuracy_gbc = accuracy_score(y_test, prediction_gbc)\n",
    "    print(\"Gradient Boosting Classifier : \", accuracy_gbc)\n",
    "\n",
    "    calculateTimeML(2, 'Gradient Boosting', accuracy_gbc)\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test, prediction_gbc))\n",
    "    \n",
    "    return (model_gbc, prediction_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOW-sJE4uMAn"
   },
   "source": [
    "### 08 : Voting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z0_hh8JSuMFD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "def trainModel_vc():\n",
    "\n",
    "    calculateTimeML(1)\n",
    "\n",
    "    clf_gbc = GradientBoostingClassifier()\n",
    "    clf_lr = LogisticRegression()\n",
    "    clf_abc = AdaBoostClassifier()\n",
    "    model_vc = VotingClassifier(estimators=[('gbc', clf_gbc), ('lr', clf_lr), ('abc', clf_abc)], voting='soft')\n",
    "    model_vc.fit(X_train, y_train)\n",
    "    prediction_vc = model_vc.predict(X_test)\n",
    "    accuracy_vc = accuracy_score(y_test, prediction_vc)\n",
    "    print(f\"Final Accuracy Score {accuracy_vc}\")\n",
    "\n",
    "    calculateTimeML(2, 'Voting', accuracy_vc)\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test, prediction_vc))\n",
    "    \n",
    "    return (model_vc, prediction_vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModelTrainFunctions():\n",
    "    \n",
    "    model_knn, prediction_knn = trainModel_knn()\n",
    "    model_svc, prediction_svc = trainModel_svc()\n",
    "    model_rf, prediction_rf = trainModel_rf()\n",
    "    model_lr, prediction_lr = trainModel_lr()\n",
    "    model_dt, prediction_dt = trainModel_dt()\n",
    "    model_abc, prediction_abc = trainModel_abc()\n",
    "    model_gbc, prediction_gbc = trainModel_gbc()\n",
    "    model_vc, prediction_vc = trainModel_vc()\n",
    "    \n",
    "    return model_knn, model_svc, model_rf, model_lr, model_dt, model_abc, model_gbc, model_vc, prediction_knn, prediction_svc, prediction_lr, prediction_rf, prediction_dt, prediction_abc, prediction_gbc, prediction_vc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling models training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn, model_svc, model_rf, model_lr, model_dt, model_abc, model_gbc, model_vc, prediction_knn, prediction_svc, prediction_lr, prediction_rf, prediction_dt, prediction_abc, prediction_gbc,prediction_vc = runModelTrainFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The total processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zfBsP_kAtg3X"
   },
   "outputs": [],
   "source": [
    "print(f'Total processing time : {(endTime-mainStartTime)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O8VpGpQjrSL"
   },
   "source": [
    "## < Model Evaluation >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T3ej0xnjrSL"
   },
   "source": [
    "### Evaluation Type 1 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-JSKR2gjrSL"
   },
   "source": [
    "#### Function to generate confusion matrix graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConfusionMatrixGraphs_All(attempt):\n",
    "    \n",
    "   # global prediction_knn, prediction_rf, prediction_lr, prediction_dt, prediction_abc, prediction_gbc, prediction_vc\n",
    "\n",
    "    generateConfusionMatrixGraph('KNN', y_test, prediction_knn, attempt)\n",
    "    generateConfusionMatrixGraph('SVM', y_test, prediction_svc, attempt)\n",
    "    generateConfusionMatrixGraph('Random Forest', y_test, prediction_rf, attempt)\n",
    "    generateConfusionMatrixGraph('Logistic Regression', y_test, prediction_lr, attempt)\n",
    "    generateConfusionMatrixGraph('Decision Tree', y_test, prediction_dt, attempt)\n",
    "    generateConfusionMatrixGraph('Ada Boost', y_test, prediction_abc, attempt)\n",
    "    generateConfusionMatrixGraph('Gradient Boosting Classifier', y_test, prediction_gbc, attempt)\n",
    "    generateConfusionMatrixGraph('VC Classifier', y_test, prediction_vc, attempt)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Confusion Matrix graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    generateConfusionMatrixGraphs_All(attempt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goe6LeLtjrSN"
   },
   "source": [
    "#### Function to create Confusion Matrix Arrays - Accuracy, Sensivity, Specificity, Recall & F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5-IpFQAjrSO"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix Arrays of each ML model\n",
    "\n",
    "def confusionMatrixArrays():   \n",
    "        \n",
    "        # K-Nearest Neighbors\n",
    "        cm_knn = generateConfusionMatrixArray(prediction_knn)\n",
    "\n",
    "        # Support vector machine\n",
    "        cm_svc = generateConfusionMatrixArray(prediction_svc)\n",
    "        \n",
    "        # Random forest\n",
    "        cm_rf = generateConfusionMatrixArray(prediction_rf)\n",
    "        \n",
    "        # Logistic regression\n",
    "        cm_lr = generateConfusionMatrixArray(prediction_lr)\n",
    "\n",
    "        # Decision tree\n",
    "        cm_dt = generateConfusionMatrixArray(prediction_dt)\n",
    "\n",
    "        # Ada boost\n",
    "        cm_abc = generateConfusionMatrixArray(prediction_abc)\n",
    "\n",
    "        # Gradient boosting\n",
    "        cm_gbc = generateConfusionMatrixArray(prediction_gbc)\n",
    "\n",
    "        # Voting\n",
    "        cm_vc = generateConfusionMatrixArray(prediction_vc)\n",
    "\n",
    "        return (cm_knn, cm_svc, cm_rf, cm_lr, cm_dt, cm_abc, cm_gbc, cm_vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to create Confusion Matrix Arrays - Accuracy, Sensivity, Specificity, Recall & F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_knn, cm_svc, cm_rf, cm_lr, cm_dt, cm_abc, cm_gbc, cm_vc = confusionMatrixArrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0An5e3zjrSO"
   },
   "source": [
    "#### Function to append 'df_ml_eval' DataFrame with sensivity and specificity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendConfusionMatrixResults():\n",
    "    \n",
    "    df_ml_eval['TP'] = [cm_knn[0][0], cm_svc[0][0], cm_rf[0][0], cm_lr[0][0], cm_dt[0][0], cm_abc[0][0], cm_gbc[0][0], cm_vc[0][0]]\n",
    "    df_ml_eval['TN'] = [cm_knn[1][1], cm_svc[1][1], cm_rf[1][1], cm_lr[1][1], cm_dt[1][1], cm_abc[1][1], cm_gbc[1][1], cm_vc[1][1]]\n",
    "    df_ml_eval['FP'] = [cm_knn[1][0], cm_svc[1][0], cm_rf[1][0], cm_lr[1][0], cm_dt[1][0], cm_abc[1][0], cm_gbc[1][0], cm_vc[1][0]]\n",
    "    df_ml_eval['FN'] = [cm_knn[0][1], cm_svc[0][1], cm_rf[0][1], cm_lr[0][1], cm_dt[0][1], cm_abc[0][1], cm_gbc[0][1], cm_vc[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to append 'df_ml_eval' DataFrame with sensivity and specificity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendConfusionMatrixResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo9Kl7q3jrSP"
   },
   "source": [
    "#### Function to calculating accuracy, sensitivity, specificity, recall & F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TOGf7E3jrSP"
   },
   "outputs": [],
   "source": [
    "def calculateAccSensiSpeciRecallF1Score():\n",
    "\n",
    "    # The total of predicted positives and predicted negatives of all the predictions\n",
    "    df_ml_eval['ACCURACY'] = round(((df_ml_eval['TP']+df_ml_eval['TN'])/(df_ml_eval['TP'] + df_ml_eval['TN'] + df_ml_eval['FP']+df_ml_eval['FN'])*100), 2)\n",
    "\n",
    "    # Precision : Sensitivity - Predicted true positives by predicted total positives\n",
    "    df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED'] = round((df_ml_eval['TP']/(df_ml_eval['TP']+df_ml_eval['FN']))*100, 2)\n",
    "\n",
    "    # Precision : Sepcificity - Predicted true negatives by predicted total negatives\n",
    "    df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'] = round((df_ml_eval['TN']/(df_ml_eval['TN']+df_ml_eval['FP']))*100, 2)\n",
    "    \n",
    "    # Precision\n",
    "    df_ml_eval['PRECISION'] = round(df_ml_eval['TP'] / (df_ml_eval['TP'] + df_ml_eval['FP']) * 100, 2)\n",
    "\n",
    "    # F1-Score\n",
    "    df_ml_eval['F1_SCORE'] = round(((df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'] * df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED']) / (df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'] + df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED']))*2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function for calculating accuracy, sensitivity, specificity, recall & F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateAccSensiSpeciRecallF1Score()\n",
    "\n",
    "df_ml_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7qJLw9hjrSQ"
   },
   "source": [
    "#### Function for plotting accuracy, sensitivity, specificity, recall, F1_score & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_Pq92QmjrSQ"
   },
   "outputs": [],
   "source": [
    "def modelComparisionPlot(fName):\n",
    "\n",
    "    fig, ((ax0, ax1), (ax2, ax3), (ax4, ax5)) = plt.subplots(ncols=2, nrows=3, figsize=(25, 13), sharex=True)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    bar_container0 = ax0.bar(df_ml_eval['ALGORITHM'], df_ml_eval['ACCURACY'])\n",
    "    ax0.bar_label(bar_container0)\n",
    "    ax0.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['ACCURACY'])\n",
    "    ax0.plot(df_ml_eval['ALGORITHM'], df_ml_eval['ACCURACY'])\n",
    "    ax0.set(title='Accuracy Graph');\n",
    "    ax0.grid(True)\n",
    "\n",
    "    bar_container1 = ax1.bar(df_ml_eval['ALGORITHM'], df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED'])\n",
    "    ax1.bar_label(bar_container1)\n",
    "    ax1.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED'])\n",
    "    ax1.plot(df_ml_eval['ALGORITHM'], df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED'])\n",
    "    ax1.set(title='Sensitivity Graph - Churn NO Identified' )\n",
    "    ax1.grid(True)\n",
    "\n",
    "    bar_container2 = ax2.bar(df_ml_eval['ALGORITHM'], df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'])\n",
    "    ax2.bar_label(bar_container2)\n",
    "    ax2.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'])\n",
    "    ax2.plot(df_ml_eval['ALGORITHM'], df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED'])\n",
    "    ax2.set(title='Specificity Graph - Churn YES Identified')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    bar_container3 = ax3.bar(df_ml_eval['ALGORITHM'], df_ml_eval['PRECISION'])\n",
    "    ax3.bar_label(bar_container3)\n",
    "    ax3.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['PRECISION'])\n",
    "    ax3.plot(df_ml_eval['ALGORITHM'], df_ml_eval['PRECISION'])\n",
    "    ax3.set(title='Recall Graph')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    bar_container4 = ax4.bar(df_ml_eval['ALGORITHM'], df_ml_eval['F1_SCORE'])\n",
    "    ax4.bar_label(bar_container4)\n",
    "    ax4.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['F1_SCORE'])\n",
    "    ax4.plot(df_ml_eval['ALGORITHM'], df_ml_eval['F1_SCORE'])\n",
    "    ax4.set(title='F1-Score Graph')\n",
    "    ax4.grid(True)\n",
    "\n",
    "    bar_container5 = ax5.bar(df_ml_eval['ALGORITHM'], df_ml_eval['TIME'])\n",
    "    ax5.bar(df_ml_eval['ALGORITHM'], df_ml_eval['TIME'])\n",
    "    #ax5.bar_label(bar_container1)\n",
    "    ax5.set(title='Processed Time', xlabel='', ylabel='Time (seconds)')\n",
    "    ax5.grid(True)\n",
    "    plt.xticks(rotation=70);\n",
    "\n",
    "    fig.suptitle(fName, fontsize = 16, weight = 'extra bold', y=1)\n",
    "    plt.savefig(f'{fName}.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function for plotting accuracy, sensitivity, specificity, recall, F1_score & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComparisionPlot('Bar Graphs - Before Columns Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval['CUMULATIVE'] = (df_ml_eval['ACCURACY']+df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED']+\n",
    "                                      df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED']+df_ml_eval['PRECISION']+\n",
    "                                      df_ml_eval['F1_SCORE'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cumulativeModelAccuracyGraph():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    bar_container = ax.bar(df_ml_eval['ALGORITHM'], df_ml_eval['CUMULATIVE'])\n",
    "    ax.bar_label(bar_container)\n",
    "    ax.scatter(df_ml_eval['ALGORITHM'], df_ml_eval['CUMULATIVE'])\n",
    "    ax.plot(df_ml_eval['ALGORITHM'], df_ml_eval['CUMULATIVE'])\n",
    "    ax.set(title='Cumulative Graph')\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativeModelAccuracyGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnGenerateHeatMap():\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "    sns.heatmap(df[:].corr(),annot = True,fmt='.1g',linecolor='white',cmap=\"YlGnBu\",linewidths=.5)\n",
    "    plt.title(\"Heatmap\",fontsize= 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnGenerateHeatMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMSKBJiWjrSQ"
   },
   "source": [
    "### Evaluation Type 2 : Receiver Operating Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyn_qfXQjrSQ"
   },
   "source": [
    "#### Function to generating ROC graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaCLj6BajrSQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def generateROCgraphs(fName):\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=4, figsize=(10, 12))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    models = [model_knn, model_svc, model_rf, model_lr, model_dt, model_abc, model_gbc, model_vc]\n",
    "    modelN = ['KNN', 'SVM', 'Random Forest', 'Logistic Regression', 'Decision Tree', 'Ada Boost', 'Gradient Boosting', 'VC']\n",
    "\n",
    "    j = 0\n",
    "    k = 0\n",
    "    \n",
    "    colors = ['red','green','blue','yellow', 'brown', 'black', 'orange', 'pink']\n",
    "\n",
    "    for i in range(len(models)):\n",
    "\n",
    "        y_pred_prob = models[i].predict_proba(X_test)[:,1]\n",
    "        fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "        ax[j][k].plot([0, 1], [0, 1], 'k--' )\n",
    "        ax[j][k].plot(fpr_rf, tpr_rf, label=modelN[i],color = \"r\")\n",
    "        ax[j][k].set(title=f'{modelN[i]} ROC Curve', xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "\n",
    "        if k == 1:\n",
    "            j = j + 1\n",
    "            k = 0\n",
    "            continue\n",
    "\n",
    "        k = k + 1\n",
    "\n",
    "    \n",
    "    fig.suptitle(fName, fontsize = 16, weight = 'extra bold', y=0.9)\n",
    "    plt.savefig(f'{fName}.png', dpi=100)\n",
    " \n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    l = 0    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "\n",
    "        y_pred_prob = models[i].predict_proba(X_test)[:,1]\n",
    "        fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], 'k--' )\n",
    "        ax.plot(fpr_rf, tpr_rf, label=modelN[i],color = colors[l])\n",
    "        ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate')       \n",
    "        ax.legend()\n",
    "\n",
    "        l = l + 1\n",
    "    \n",
    "    fig.suptitle('All ROC Curves', fontsize = 16, weight = 'extra bold', y=1)\n",
    "    plt.savefig(f'{fName}.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to generate ROC graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateROCgraphs('ROC Curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "feature_importances = model_abc.feature_importances_\n",
    "\n",
    "# Sort and rank features by importance\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Important columns\n",
    "colNamesToBeRemoved = []\n",
    "\n",
    "# Print the ranked list of feature names and their importances\n",
    "for idx in sorted_indices:\n",
    "    if feature_importances[idx] == 0.0:\n",
    "        colNamesToBeRemoved.append(df.columns[idx])\n",
    "        print(f\"Feature: {df.columns[idx]}, Importance: {feature_importances[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By analyzing the importance of the features, the below columns can be removed from the dataset and build the models for the second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNamesToBeRemoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(colNamesToBeRemoved, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnGenerateHeatMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping columns that have low correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colNamesToBeRemoved.clear()\n",
    "# colNamesToBeRemoved = ['CallForwardingCalls', 'RVOwner', 'ReferralsMadeBySubscriber', 'MadeCallToRetentionTeam',\n",
    "#                        'PrizmCode_Rural', 'Occupation_Homemaker', 'MaritalStatus_No']\n",
    "\n",
    "# df.drop(colNamesToBeRemoved, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the models after removing some features : Analiyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View final profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showViz == 1:\n",
    "    runProfilingReport('Final Data Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning results dataframe re-initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval.drop(df_ml_eval.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval = pd.DataFrame({'ATTEMPT':[], 'ALGORITHM':[], 'TIME':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-initializing the X_train, X_test, y_train, y_test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_test = df.drop(df.index)\n",
    "y_train = y_test = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitDataset(df)\n",
    "replaceYesNo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling models training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn, model_svc, model_rf, model_lr, model_dt, model_abc, model_gbc, model_vc, prediction_knn, prediction_svm, prediction_lr, prediction_rf, prediction_dt, prediction_abc, prediction_gbc, prediction_vc = runModelTrainFunctions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Confusion Matrix graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateConfusionMatrixGraphs_All(attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to create Confusion Matrix Arrays - Accuracy, Sensivity, Specificity, Recall & F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_knn, cm_svc, cm_rf, cm_lr, cm_dt, cm_abc, cm_gbc, cm_vc = confusionMatrixArrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_knn, cm_svc, cm_rf, cm_lr, cm_dt, cm_abc, cm_gbc, cm_vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to append 'df_ml_eval' DataFrame with sensivity and specificity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendConfusionMatrixResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function for calculating accuracy, sensitivity, specificity, recall & F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateAccSensiSpeciRecallF1Score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function for plotting accuracy, sensitivity, specificity, recall, F1_score & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComparisionPlot('Bar Graph - After Columns Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_eval['CUMULATIVE'] = (df_ml_eval['ACCURACY']+df_ml_eval['SENSITIVITY-RECALL-CHURN_NO_IDENTIFIED']+df_ml_eval['SPECIFICITY-CHURN_YES_IDENTIFIED']+df_ml_eval['PRECISION']+df_ml_eval['F1_SCORE'])/5\n",
    "df_ml_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativeModelAccuracyGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function to generate ROC graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateROCgraphs('ROC Curves - After Columns Removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the correlations of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categoricals\n",
    "categoricals = list()\n",
    "for x in df.columns:\n",
    "    if df[x].dtype == 'object':\n",
    "        categoricals.append(x)\n",
    "df[categoricals].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = [x for x in df.columns if x not in categoricals]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "df[numericals].corr(method='pearson')['Churn'].sort_values(ascending = False).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVDWz0n3jrSR"
   },
   "source": [
    "## < Finalizing The Most Suitable Model >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aScoW0n-jrSR"
   },
   "source": [
    "## < Model Deployment >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKHZokg8jrSR"
   },
   "source": [
    "### Importing the 'joblib' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRteQ8DKjrSR"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdHqODVxjrSR"
   },
   "source": [
    "### Save the model as a file\n",
    "\n",
    "This exported trained model can be used to predict customer churns in the telcom organization. By comparing the analyzed data, \n",
    "the 'Ada Boosting Classifier' model has perfomed well in this dataset. Therefore, 'model_abc' is exported as the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpXjkiStjrSS"
   },
   "outputs": [],
   "source": [
    "joblib.dump(model_abc, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation : Predicting the Churns of a Given Dataset Using the Deployed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgXrCN8qjrSS"
   },
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTFJNlZvjrSS"
   },
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxCxZiAHjrSS"
   },
   "source": [
    "### Function to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFA-EPsejrST"
   },
   "outputs": [],
   "source": [
    "def fnPredict():\n",
    "\n",
    "    global dfHoldout\n",
    "\n",
    "    # Drop 'Churn' column from the dataset\n",
    "    dfHoldout.drop('Churn Value', axis=1, inplace=True)\n",
    "\n",
    "    # Drop 'City' column\n",
    "    dfHoldout.drop('City', axis=1, inplace=True)\n",
    "    \n",
    "    # Apply the custom functions to prepare the dataset for the deployed ML model\n",
    "    \n",
    "    # Encoding\n",
    "    dfHoldout = labelEncoding(dfHoldout)\n",
    "    dfHoldout = oneHotEncoding(dfHoldout)\n",
    "    \n",
    "    # Drop Low correlation features\n",
    "    dfHoldout.drop(['Churn Label','CustomerID','Country','State','Lat Long','Gender','Phone Service','Total Charges',\n",
    "                    'Churn Reason','Count','Zip Code','Latitude','Longitude', 'Churn Score'], axis=1, inplace=True)\n",
    "    \n",
    "    # Low importance features\n",
    "    dfHoldout.drop(['Partner',  'Dependents',  'Payment Method_Electronic check', 'Online Backup', 'Device Protection',\n",
    "                    'Internet Service_Fiber optic', 'Internet Service_No', 'Payment Method_Bank transfer (automatic)', \n",
    "                    'Senior Citizen'], axis=1, inplace=True)\n",
    "    # Scaling\n",
    "    minMaxScale(dfHoldout)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(dfHoldout)\n",
    "\n",
    "    # Move predicitions in a new column 'Churn'\n",
    "    dfHoldout['Churn'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n416tEPTjrST"
   },
   "source": [
    "### Read the data to be prodicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWS03dQmjrST"
   },
   "outputs": [],
   "source": [
    "# There are 502 number of records in the dataset to be predicted\n",
    "dfHoldout = pd.read_csv('CustomerChurnDataset_Holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldout.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSMgx0Q1jrST"
   },
   "source": [
    "### Call the 'fnPredict' function and assign the new dataset with the new 'Churn' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhJapLY4jrSU"
   },
   "outputs": [],
   "source": [
    "fnPredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldout.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgYNcHPxjrSU"
   },
   "source": [
    "### View the predicted churn column of the  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0u03zVStjrSU"
   },
   "outputs": [],
   "source": [
    "dfHoldout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total time : {(datetime.now()-mainStartTime)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF THE CODE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
